{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Exploration\n",
    "There are several commands in pandas that you can use in the begining of a exploratory data analysis\n",
    "\n",
    "Before start cleaning data, grouping, managing missing values, it is important to know some basic aspects of your dataset:\n",
    "* Dimensions\n",
    "* Columns\n",
    "* Few observations\n",
    "* Types of data\n",
    "* Missings\n",
    "* Some statistics\n",
    "\n",
    "And you can achieve this in just a few lines. First, let's start loading a dataset. In this case, the 'mpg' dataset from the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions\n",
    "You can calculate the dimensions of the dataset with the `shape` attribute. It returns a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns\n",
    "If you want to know the columns of the dataset, just use the sintaxis `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also know the amount of columns of the dataset with the `len` of `df.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few observations\n",
    "If you want to check in a few registers the aspect of the dataset's rows, you can use `df.head()` or `df.tail()`. It has an argument for specifying the numer of observations you want to see, 5 by default. Head for the first ones and tail for the last ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of data\n",
    "It is very important to know the types of every column because you will apply feature engineering and make some transformations in the datasset, and if you don't know the types of the columns, the data wrangling won't be applied correctly. So, how can we know the types of the data? with `df.dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the types of the data are the same as numpy types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missings\n",
    "Another useful information you must know at the beginning of the exploratory analysis is the amount of null values per column. You can achieve this with `df.info()`. Info is a very useful method because there is not only the information about missings, but also the types of data and amount of columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics\n",
    "We have just seen some metadata about the DataFrama, and even a few observations with `head()` or `tail()`, but some aggregated information about the columns coud be very useful for understanding their the behaviour. We can do this with the method `df.describe()`. By default, it will print a DataFrame with basic statistics of numeric variables (max, mean, min, count and percentiles).\n",
    "\n",
    "[Here is a link to the documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see both numerical and categorical columns with the argument `include=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not all the information that we can get from a pandas DataFrame, this notebook is composed with just a few useful lines of code that you must use when import a new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
